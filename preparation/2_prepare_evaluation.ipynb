{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Adversarial tests: Prepare TSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate AAA files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prl222/OneDrive-University/multidisciplinary-data-challenge/preparation/Adversifier\n",
      "/Users/prl222/OneDrive-University/multidisciplinary-data-challenge/preparation/Adversifier/mhs\n"
     ]
    }
   ],
   "source": [
    "test_names = ['quoting_a_to_n', 'flip_n_to_a', 'corr_a_to_a', 'corr_n_to_n']\n",
    "other = ['f1_o', 'hashtag_check']\n",
    "\n",
    "os.chdir('Adversifier')\n",
    "!pwd\n",
    "!../gen_variables.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script runs command with bind to generate the files.\n",
    "# !docker run --mount type=bind,source=$AAA_FILE_DIR,target=/aaa/input aaa python3 gen.py --dataset_name $DATASET_NAME --train $TRAINING_SET --test $TEST_SET"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "where $AAA_FILE_DIR is the absolute path to the directory containing your datasets (for example, \"$(pwd)\"/mydata), \n",
    "$TRAINING_SET and $TEST_SET are the filenames of the training and test data files (placed inside $AAA_FILE_DIR), \n",
    "and $DATASET_NAME is a string identifier for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save without ground truth to the challenge folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- quoting_a_to_n: all abusive test posts and include in template (counter speech)\n",
    "- flip_n_to_a: all non abusive test posts and append an abusive test post.\n",
    "- corr_a_to_a: append $k$ most common non-abusive expressions from the training (the post is still abusive)\n",
    "- corr_n_to_n: append most common from abusive test examples (which are not discarded using the lemmatized version on Hurtlex, so that appended words do not change into abusive. E.g., transgender, black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3392, 2)\n",
      "(9665, 2)\n",
      "(3392, 2)\n",
      "(9665, 2)\n",
      "(13057, 2)\n",
      "(13057, 2)\n"
     ]
    }
   ],
   "source": [
    "# Export the evaluation files\n",
    "data_path = 'Adversifier/mhs/aaa_files'\n",
    "for test_name in test_names+other:\n",
    "    df = pd.read_csv(os.path.join(data_path, 'aaa_files', test_name+'.tsv'),  sep ='\\t', names=['text', 'gtruth'])\n",
    "    print(df.shape)\n",
    "    df['text'].to_csv(os.path.join('../challenge/eval_files', test_name+'.tsv'),  sep ='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Adversarial (static) tests: Prepare TSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hatecheck-experiments'...\n",
      "remote: Enumerating objects: 89, done.\u001b[K\n",
      "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
      "remote: Total 89 (delta 36), reused 38 (delta 12), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (89/89), 21.94 MiB | 2.78 MiB/s, done.\n",
      "Resolving deltas: 100% (36/36), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/paul-rottger/hatecheck-experiments.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export the test_case and label_gold (without columns) from hatecheck_final_ACL and save as hatecheck.csv in mhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3728, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionality</th>\n",
       "      <th>case_id</th>\n",
       "      <th>test_case</th>\n",
       "      <th>label_gold</th>\n",
       "      <th>target_ident</th>\n",
       "      <th>direction</th>\n",
       "      <th>focus_words</th>\n",
       "      <th>focus_lemma</th>\n",
       "      <th>ref_case_id</th>\n",
       "      <th>ref_templ_id</th>\n",
       "      <th>templ_id</th>\n",
       "      <th>case_templ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate women.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>women</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>2</td>\n",
       "      <td>I hate trans people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>3</td>\n",
       "      <td>I hate gay people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>gay people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>4</td>\n",
       "      <td>I hate black people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>black people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>5</td>\n",
       "      <td>I hate disabled people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>disabled people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       functionality  case_id                 test_case label_gold  \\\n",
       "0  derog_neg_emote_h        1            I hate women.     hateful   \n",
       "1  derog_neg_emote_h        2     I hate trans people.     hateful   \n",
       "2  derog_neg_emote_h        3       I hate gay people.     hateful   \n",
       "3  derog_neg_emote_h        4     I hate black people.     hateful   \n",
       "4  derog_neg_emote_h        5  I hate disabled people.     hateful   \n",
       "\n",
       "      target_ident direction focus_words focus_lemma  ref_case_id  \\\n",
       "0            women   general        hate        hate          NaN   \n",
       "1     trans people   general        hate        hate          NaN   \n",
       "2       gay people   general        hate        hate          NaN   \n",
       "3     black people   general        hate        hate          NaN   \n",
       "4  disabled people   general        hate        hate          NaN   \n",
       "\n",
       "   ref_templ_id  templ_id            case_templ  \n",
       "0           NaN         1  I hate [IDENTITY_P].  \n",
       "1           NaN         1  I hate [IDENTITY_P].  \n",
       "2           NaN         1  I hate [IDENTITY_P].  \n",
       "3           NaN         1  I hate [IDENTITY_P].  \n",
       "4           NaN         1  I hate [IDENTITY_P].  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Challeng_PATH = os.path.join(os.getcwd(), 'challenge', 'evaluation_files')\n",
    "\n",
    "H_PATH = 'hatecheck-experiments/Data/Test Suite'\n",
    "hatecheck_df = pd.read_csv(os.path.join(H_PATH,'hatecheck_final_ACL.csv'), index_col=0)\n",
    "\n",
    "print(hatecheck_df.shape)\n",
    "hatecheck_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatecheck_df['test_case'].to_csv(os.path.join(os.getcwd(), '../challenge/eval_files', 'static.tsv'),  sep ='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a submission with random predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3392, 1)\n",
      "(9665, 1)\n",
      "(3392, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/yt9w9cy56s9_61qd0vmx3v6hn7v97j/T/ipykernel_64563/1160069320.py:14: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "  df['predict'] = np.random.random_integers(low=0, high=1, size=df.shape[0])\n",
      "/var/folders/59/yt9w9cy56s9_61qd0vmx3v6hn7v97j/T/ipykernel_64563/1160069320.py:14: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "  df['predict'] = np.random.random_integers(low=0, high=1, size=df.shape[0])\n",
      "/var/folders/59/yt9w9cy56s9_61qd0vmx3v6hn7v97j/T/ipykernel_64563/1160069320.py:14: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "  df['predict'] = np.random.random_integers(low=0, high=1, size=df.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9665, 1)\n",
      "(13057, 1)\n",
      "(13057, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/yt9w9cy56s9_61qd0vmx3v6hn7v97j/T/ipykernel_64563/1160069320.py:14: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "  df['predict'] = np.random.random_integers(low=0, high=1, size=df.shape[0])\n",
      "/var/folders/59/yt9w9cy56s9_61qd0vmx3v6hn7v97j/T/ipykernel_64563/1160069320.py:14: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "  df['predict'] = np.random.random_integers(low=0, high=1, size=df.shape[0])\n",
      "/var/folders/59/yt9w9cy56s9_61qd0vmx3v6hn7v97j/T/ipykernel_64563/1160069320.py:14: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "  df['predict'] = np.random.random_integers(low=0, high=1, size=df.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3728, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/yt9w9cy56s9_61qd0vmx3v6hn7v97j/T/ipykernel_64563/1160069320.py:14: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "  df['predict'] = np.random.random_integers(low=0, high=1, size=df.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Example submissions\n",
    "os.mkdir('Adversifier/datathon_results')\n",
    "os.mkdir('Adversifier/datathon_results/team_random')\n",
    "\n",
    "eval_files = ['quoting_a_to_n', 'flip_n_to_a', 'corr_a_to_a', 'corr_n_to_n', \n",
    "              'f1_o', 'hashtag_check', 'static']\n",
    "for eval_f in eval_files:\n",
    "    # load from evaluation files folder\n",
    "    df = pd.read_csv(os.path.join('../challenge/eval_files', eval_f+'.tsv'),  \n",
    "                    sep ='\\t', \n",
    "                    names=['text'])\n",
    "    print(df.shape)\n",
    "    # add model predictions and binarize (1=hateful, 0=non-hateful)\n",
    "    df['predict'] = np.random.random_integers(low=0, high=1, size=df.shape[0])\n",
    "    # export to team folder\n",
    "    df.to_csv(os.path.join('Adversifier', 'datathon_results', 'predictions', f'team_random', eval_f+'.tsv'),  \n",
    "                    sep ='\\t', \n",
    "                    index=False, \n",
    "                    header=False)\n",
    "# then, submit team_X folder through teams\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-challenge",
   "language": "python",
   "name": "data-challenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
